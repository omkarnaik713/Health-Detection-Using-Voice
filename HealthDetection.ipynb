{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-23 10:47:07.970640: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-07-23 10:47:07.973902: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-07-23 10:47:07.985947: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-23 10:47:08.008586: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-23 10:47:08.008627: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-23 10:47:08.024397: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-23 10:47:09.055350: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/dylan/HealthDetectionThroughVoice/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import librosa\n",
    "import tensorflow as tf \n",
    "from tensorflow.keras.layers import Dense, Dropout,Input\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam,SGD\n",
    "from transformers import  WhisperProcessor\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import metrics \n",
    "from sklearn.model_selection import train_test_split\n",
    "import mlflow\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/07/23 10:48:17 INFO mlflow.tracking.fluent: Experiment with name 'Using MFCC from Librosa' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='mlflow-artifacts:/670346074458369401', creation_time=1721746097304, experiment_id='670346074458369401', last_update_time=1721746097304, lifecycle_stage='active', name='Using MFCC from Librosa', tags={}>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.set_tracking_uri('http://127.0.0.1:5000')\n",
    "mlflow.set_experiment('Using MFCC from Librosa')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/07/19 07:49:38 WARNING mlflow.utils.autologging_utils: You are using an unsupported version of tensorflow. If you encounter errors during autologging, try upgrading / downgrading tensorflow to a supported version, or try upgrading MLflow.\n"
     ]
    }
   ],
   "source": [
    "mlflow.tensorflow.autolog()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "\n",
    "folder_path = '/home/dylan/HealthDetectionThroughVoice/patient-vocal-dataset'\n",
    "label = []\n",
    "k =0\n",
    "audio_data = []\n",
    "\n",
    "for folder in os.listdir(folder_path):\n",
    "    audio_folder_path = os.path.join(folder_path,folder)\n",
    "    for filename in os.listdir(audio_folder_path):\n",
    "        if filename.endswith('.wav') :\n",
    "            audio_file_path = os.path.join(audio_folder_path,filename)\n",
    "            audio,_ = librosa.load(audio_file_path,sr = 16000)\n",
    "            audio_data.append(audio)\n",
    "            label.append(k)\n",
    "    k += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Normal', 'Vox senilis', 'Laryngozele']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction(audio,sr):\n",
    "    #features = feature_extractor(audio,sampling_rate=sr,return_tensors='np').input_features[0]\n",
    "    features = librosa.feature.mfcc(y = audio, sr = sr,n_mfcc= 50)\n",
    "    transpose_feature = np.transpose(features,(1,0))\n",
    "    average_scaled_features = np.mean(transpose_feature, axis = 0)\n",
    "    return average_scaled_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []\n",
    "for audio in audio_data :\n",
    "    features.append(feature_extraction(audio,_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1036"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(data = features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['labels'] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['labels'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data.drop('labels', axis =1)\n",
    "y = np.array(data['labels'].tolist())\n",
    "y = np.array(pd.get_dummies(y))\n",
    "X_train,X_test,y_train,y_test = train_test_split(x,y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-160.685165</td>\n",
       "      <td>45.490135</td>\n",
       "      <td>-4.821774</td>\n",
       "      <td>35.931957</td>\n",
       "      <td>-10.787026</td>\n",
       "      <td>15.464179</td>\n",
       "      <td>-8.226496</td>\n",
       "      <td>6.952154</td>\n",
       "      <td>-5.721563</td>\n",
       "      <td>1.627728</td>\n",
       "      <td>...</td>\n",
       "      <td>-21.462183</td>\n",
       "      <td>9.511221</td>\n",
       "      <td>12.668848</td>\n",
       "      <td>-13.635872</td>\n",
       "      <td>-23.943476</td>\n",
       "      <td>5.275884</td>\n",
       "      <td>11.726404</td>\n",
       "      <td>-15.634768</td>\n",
       "      <td>-29.240330</td>\n",
       "      <td>-1.091166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-312.905090</td>\n",
       "      <td>106.385986</td>\n",
       "      <td>30.501062</td>\n",
       "      <td>31.441315</td>\n",
       "      <td>-9.121834</td>\n",
       "      <td>-9.406259</td>\n",
       "      <td>-3.091830</td>\n",
       "      <td>4.112461</td>\n",
       "      <td>-11.861338</td>\n",
       "      <td>0.447344</td>\n",
       "      <td>...</td>\n",
       "      <td>2.208146</td>\n",
       "      <td>1.817251</td>\n",
       "      <td>-0.181122</td>\n",
       "      <td>0.120193</td>\n",
       "      <td>-0.497871</td>\n",
       "      <td>2.098290</td>\n",
       "      <td>-1.448841</td>\n",
       "      <td>-1.255415</td>\n",
       "      <td>-1.664657</td>\n",
       "      <td>-2.020061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-198.047699</td>\n",
       "      <td>144.320862</td>\n",
       "      <td>17.916195</td>\n",
       "      <td>21.477489</td>\n",
       "      <td>10.471588</td>\n",
       "      <td>13.473979</td>\n",
       "      <td>2.826607</td>\n",
       "      <td>7.292999</td>\n",
       "      <td>1.832671</td>\n",
       "      <td>5.262392</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.524959</td>\n",
       "      <td>0.355890</td>\n",
       "      <td>-3.344779</td>\n",
       "      <td>-0.008432</td>\n",
       "      <td>-0.059242</td>\n",
       "      <td>5.186929</td>\n",
       "      <td>5.414943</td>\n",
       "      <td>8.438732</td>\n",
       "      <td>6.083746</td>\n",
       "      <td>7.030056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-277.448975</td>\n",
       "      <td>88.948769</td>\n",
       "      <td>29.334581</td>\n",
       "      <td>24.219416</td>\n",
       "      <td>12.109037</td>\n",
       "      <td>5.466475</td>\n",
       "      <td>-8.748367</td>\n",
       "      <td>14.119501</td>\n",
       "      <td>-14.876341</td>\n",
       "      <td>-8.853692</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.082283</td>\n",
       "      <td>3.768034</td>\n",
       "      <td>0.810410</td>\n",
       "      <td>4.173285</td>\n",
       "      <td>0.778366</td>\n",
       "      <td>4.612110</td>\n",
       "      <td>-0.353598</td>\n",
       "      <td>3.696862</td>\n",
       "      <td>2.749008</td>\n",
       "      <td>5.950413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-298.540070</td>\n",
       "      <td>158.716324</td>\n",
       "      <td>29.298662</td>\n",
       "      <td>-3.409457</td>\n",
       "      <td>-25.199650</td>\n",
       "      <td>-1.150077</td>\n",
       "      <td>-11.963250</td>\n",
       "      <td>-0.317667</td>\n",
       "      <td>4.107648</td>\n",
       "      <td>-6.450688</td>\n",
       "      <td>...</td>\n",
       "      <td>40.711491</td>\n",
       "      <td>51.524853</td>\n",
       "      <td>30.746332</td>\n",
       "      <td>15.714661</td>\n",
       "      <td>-3.517975</td>\n",
       "      <td>-1.933229</td>\n",
       "      <td>2.197755</td>\n",
       "      <td>5.977923</td>\n",
       "      <td>-0.656958</td>\n",
       "      <td>-6.218029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1031</th>\n",
       "      <td>-249.958893</td>\n",
       "      <td>122.502533</td>\n",
       "      <td>73.687592</td>\n",
       "      <td>-14.402223</td>\n",
       "      <td>11.862350</td>\n",
       "      <td>-20.625967</td>\n",
       "      <td>-11.463581</td>\n",
       "      <td>25.574898</td>\n",
       "      <td>3.157075</td>\n",
       "      <td>-3.082764</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.997714</td>\n",
       "      <td>-6.161963</td>\n",
       "      <td>1.260105</td>\n",
       "      <td>-2.781147</td>\n",
       "      <td>7.297731</td>\n",
       "      <td>12.011082</td>\n",
       "      <td>15.292834</td>\n",
       "      <td>25.896961</td>\n",
       "      <td>11.474255</td>\n",
       "      <td>17.393736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1032</th>\n",
       "      <td>-211.993591</td>\n",
       "      <td>55.510059</td>\n",
       "      <td>-40.787830</td>\n",
       "      <td>93.921974</td>\n",
       "      <td>19.720709</td>\n",
       "      <td>-42.127529</td>\n",
       "      <td>-7.178773</td>\n",
       "      <td>-0.600349</td>\n",
       "      <td>-25.551603</td>\n",
       "      <td>-15.691953</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.576841</td>\n",
       "      <td>-6.795305</td>\n",
       "      <td>-8.308597</td>\n",
       "      <td>9.469365</td>\n",
       "      <td>7.675497</td>\n",
       "      <td>-3.163553</td>\n",
       "      <td>-8.838223</td>\n",
       "      <td>-3.405991</td>\n",
       "      <td>3.822929</td>\n",
       "      <td>5.032875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1033</th>\n",
       "      <td>-137.966064</td>\n",
       "      <td>135.945160</td>\n",
       "      <td>-12.244995</td>\n",
       "      <td>-47.050213</td>\n",
       "      <td>2.576136</td>\n",
       "      <td>-12.112126</td>\n",
       "      <td>-4.900474</td>\n",
       "      <td>11.373034</td>\n",
       "      <td>3.213206</td>\n",
       "      <td>-4.643087</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.791137</td>\n",
       "      <td>-1.471322</td>\n",
       "      <td>-7.373990</td>\n",
       "      <td>-0.827134</td>\n",
       "      <td>-11.440187</td>\n",
       "      <td>-0.615440</td>\n",
       "      <td>-5.766508</td>\n",
       "      <td>-5.503603</td>\n",
       "      <td>-2.432851</td>\n",
       "      <td>-3.399486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1034</th>\n",
       "      <td>-168.714355</td>\n",
       "      <td>82.739861</td>\n",
       "      <td>24.001690</td>\n",
       "      <td>17.513767</td>\n",
       "      <td>6.689113</td>\n",
       "      <td>4.423263</td>\n",
       "      <td>7.146572</td>\n",
       "      <td>7.735805</td>\n",
       "      <td>5.888155</td>\n",
       "      <td>9.388286</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.453273</td>\n",
       "      <td>0.055080</td>\n",
       "      <td>-2.465599</td>\n",
       "      <td>-1.956569</td>\n",
       "      <td>-8.699000</td>\n",
       "      <td>-9.479540</td>\n",
       "      <td>-8.938299</td>\n",
       "      <td>5.262674</td>\n",
       "      <td>21.440670</td>\n",
       "      <td>39.075733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1035</th>\n",
       "      <td>-71.719475</td>\n",
       "      <td>90.063606</td>\n",
       "      <td>26.586338</td>\n",
       "      <td>-22.830280</td>\n",
       "      <td>-43.401051</td>\n",
       "      <td>-2.021189</td>\n",
       "      <td>2.879451</td>\n",
       "      <td>23.244757</td>\n",
       "      <td>-3.887195</td>\n",
       "      <td>-3.484416</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.505727</td>\n",
       "      <td>2.340725</td>\n",
       "      <td>0.446703</td>\n",
       "      <td>3.598279</td>\n",
       "      <td>-1.355449</td>\n",
       "      <td>3.085043</td>\n",
       "      <td>0.928733</td>\n",
       "      <td>7.724566</td>\n",
       "      <td>4.778097</td>\n",
       "      <td>14.711365</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1036 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0           1          2          3          4          5   \\\n",
       "0    -160.685165   45.490135  -4.821774  35.931957 -10.787026  15.464179   \n",
       "1    -312.905090  106.385986  30.501062  31.441315  -9.121834  -9.406259   \n",
       "2    -198.047699  144.320862  17.916195  21.477489  10.471588  13.473979   \n",
       "3    -277.448975   88.948769  29.334581  24.219416  12.109037   5.466475   \n",
       "4    -298.540070  158.716324  29.298662  -3.409457 -25.199650  -1.150077   \n",
       "...          ...         ...        ...        ...        ...        ...   \n",
       "1031 -249.958893  122.502533  73.687592 -14.402223  11.862350 -20.625967   \n",
       "1032 -211.993591   55.510059 -40.787830  93.921974  19.720709 -42.127529   \n",
       "1033 -137.966064  135.945160 -12.244995 -47.050213   2.576136 -12.112126   \n",
       "1034 -168.714355   82.739861  24.001690  17.513767   6.689113   4.423263   \n",
       "1035  -71.719475   90.063606  26.586338 -22.830280 -43.401051  -2.021189   \n",
       "\n",
       "             6          7          8          9   ...         40         41  \\\n",
       "0     -8.226496   6.952154  -5.721563   1.627728  ... -21.462183   9.511221   \n",
       "1     -3.091830   4.112461 -11.861338   0.447344  ...   2.208146   1.817251   \n",
       "2      2.826607   7.292999   1.832671   5.262392  ...  -1.524959   0.355890   \n",
       "3     -8.748367  14.119501 -14.876341  -8.853692  ...  -1.082283   3.768034   \n",
       "4    -11.963250  -0.317667   4.107648  -6.450688  ...  40.711491  51.524853   \n",
       "...         ...        ...        ...        ...  ...        ...        ...   \n",
       "1031 -11.463581  25.574898   3.157075  -3.082764  ...  -5.997714  -6.161963   \n",
       "1032  -7.178773  -0.600349 -25.551603 -15.691953  ...  -0.576841  -6.795305   \n",
       "1033  -4.900474  11.373034   3.213206  -4.643087  ...  -8.791137  -1.471322   \n",
       "1034   7.146572   7.735805   5.888155   9.388286  ...  -4.453273   0.055080   \n",
       "1035   2.879451  23.244757  -3.887195  -3.484416  ...  -0.505727   2.340725   \n",
       "\n",
       "             42         43         44         45         46         47  \\\n",
       "0     12.668848 -13.635872 -23.943476   5.275884  11.726404 -15.634768   \n",
       "1     -0.181122   0.120193  -0.497871   2.098290  -1.448841  -1.255415   \n",
       "2     -3.344779  -0.008432  -0.059242   5.186929   5.414943   8.438732   \n",
       "3      0.810410   4.173285   0.778366   4.612110  -0.353598   3.696862   \n",
       "4     30.746332  15.714661  -3.517975  -1.933229   2.197755   5.977923   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "1031   1.260105  -2.781147   7.297731  12.011082  15.292834  25.896961   \n",
       "1032  -8.308597   9.469365   7.675497  -3.163553  -8.838223  -3.405991   \n",
       "1033  -7.373990  -0.827134 -11.440187  -0.615440  -5.766508  -5.503603   \n",
       "1034  -2.465599  -1.956569  -8.699000  -9.479540  -8.938299   5.262674   \n",
       "1035   0.446703   3.598279  -1.355449   3.085043   0.928733   7.724566   \n",
       "\n",
       "             48         49  \n",
       "0    -29.240330  -1.091166  \n",
       "1     -1.664657  -2.020061  \n",
       "2      6.083746   7.030056  \n",
       "3      2.749008   5.950413  \n",
       "4     -0.656958  -6.218029  \n",
       "...         ...        ...  \n",
       "1031  11.474255  17.393736  \n",
       "1032   3.822929   5.032875  \n",
       "1033  -2.432851  -3.399486  \n",
       "1034  21.440670  39.075733  \n",
       "1035   4.778097  14.711365  \n",
       "\n",
       "[1036 rows x 50 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Run with UUID 5516b190dcd14d9bb3b3f8191ad6558e is already active. To start a new run, first end the current run with mlflow.end_run(). To start a nested run, call start_run with nested=True",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmlflow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/HealthDetectionThroughVoice/lib/python3.11/site-packages/mlflow/tracking/fluent.py:306\u001b[0m, in \u001b[0;36mstart_run\u001b[0;34m(run_id, experiment_id, run_name, nested, tags, description, log_system_metrics)\u001b[0m\n\u001b[1;32m    304\u001b[0m experiment_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(experiment_id) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(experiment_id, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m experiment_id\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(_active_run_stack) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m nested:\n\u001b[0;32m--> 306\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\n\u001b[1;32m    307\u001b[0m         (\n\u001b[1;32m    308\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRun with UUID \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m is already active. To start a new run, first end the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    309\u001b[0m             \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcurrent run with mlflow.end_run(). To start a nested \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    310\u001b[0m             \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun, call start_run with nested=True\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    311\u001b[0m         )\u001b[38;5;241m.\u001b[39mformat(_active_run_stack[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mrun_id)\n\u001b[1;32m    312\u001b[0m     )\n\u001b[1;32m    313\u001b[0m client \u001b[38;5;241m=\u001b[39m MlflowClient()\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_id:\n",
      "\u001b[0;31mException\u001b[0m: Run with UUID 5516b190dcd14d9bb3b3f8191ad6558e is already active. To start a new run, first end the current run with mlflow.end_run(). To start a nested run, call start_run with nested=True"
     ]
    }
   ],
   "source": [
    "mlflow.start_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "## adding the input layer \n",
    "model.add(Input(shape = (50,), batch_size = 16))\n",
    "## adding the first Dense Layer \n",
    "model.add(Dense(32,activation = 'relu', kernel_initializer='he_uniform', kernel_regularizer=l2(0.075)))\n",
    "## adding a dropout layer \n",
    "#model.add(Dropout(0.1))\n",
    "## layer 2\n",
    "model.add(Dense(16, activation = 'relu', kernel_initializer = 'he_uniform', kernel_regularizer=l2(0.075)))\n",
    "#model.add(Dropout(0.1))\n",
    "## layer 3\n",
    "#model.add(Dense(16, activation = 'relu', kernel_initializer='he_uniform', kernel_regularizer=l2(0.05)))\n",
    "#model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Dense(8,activation = 'relu', kernel_initializer='he_uniform', kernel_regularizer= l2(0.075)))\n",
    "#model.add(Dropout(0.5))\n",
    "#model.add(Dense(64,activation = 'relu', kernel_initializer='he_uniform'))\n",
    "\n",
    "## layer 4\n",
    "\n",
    "model.add(Dense(8,activation= 'relu', kernel_initializer = 'he_uniform', kernel_regularizer=l2(0.075)))\n",
    "#model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Dense(3, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,632</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)               │           <span style=\"color: #00af00; text-decoration-color: #00af00\">528</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)                │           <span style=\"color: #00af00; text-decoration-color: #00af00\">136</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)                │            <span style=\"color: #00af00; text-decoration-color: #00af00\">72</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                │            <span style=\"color: #00af00; text-decoration-color: #00af00\">27</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_10 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m32\u001b[0m)               │         \u001b[38;5;34m1,632\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_11 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m)               │           \u001b[38;5;34m528\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_12 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m8\u001b[0m)                │           \u001b[38;5;34m136\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_13 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m8\u001b[0m)                │            \u001b[38;5;34m72\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_14 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m3\u001b[0m)                │            \u001b[38;5;34m27\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,395</span> (9.36 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,395\u001b[0m (9.36 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,395</span> (9.36 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,395\u001b[0m (9.36 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "params={'learning_rate' : 0.005, 'epsilon' : 1e-07}\n",
    "optimizer = Adam(**params)\n",
    "model.compile(optimizer= optimizer, loss = 'categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.4848 - loss: 18.4487 - val_accuracy: 0.6058 - val_loss: 8.6563\n",
      "Epoch 2/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5816 - loss: 7.7399 - val_accuracy: 0.6250 - val_loss: 5.9393\n",
      "Epoch 3/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6299 - loss: 5.5727 - val_accuracy: 0.6587 - val_loss: 4.6533\n",
      "Epoch 4/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6749 - loss: 4.3681 - val_accuracy: 0.6971 - val_loss: 3.8225\n",
      "Epoch 5/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7227 - loss: 3.5967 - val_accuracy: 0.6923 - val_loss: 3.2175\n",
      "Epoch 6/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7266 - loss: 3.0986 - val_accuracy: 0.7212 - val_loss: 2.8539\n",
      "Epoch 7/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7509 - loss: 2.6788 - val_accuracy: 0.7212 - val_loss: 2.5470\n",
      "Epoch 8/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7437 - loss: 2.4442 - val_accuracy: 0.7308 - val_loss: 2.3181\n",
      "Epoch 9/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7430 - loss: 2.2201 - val_accuracy: 0.7163 - val_loss: 2.2601\n",
      "Epoch 10/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7597 - loss: 2.0442 - val_accuracy: 0.7308 - val_loss: 2.0091\n",
      "Epoch 11/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7688 - loss: 1.9173 - val_accuracy: 0.7260 - val_loss: 1.9184\n",
      "Epoch 12/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7438 - loss: 1.8307 - val_accuracy: 0.7308 - val_loss: 1.8300\n",
      "Epoch 13/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7857 - loss: 1.6860 - val_accuracy: 0.7404 - val_loss: 1.7055\n",
      "Epoch 14/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7484 - loss: 1.6573 - val_accuracy: 0.7452 - val_loss: 1.6363\n",
      "Epoch 15/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7643 - loss: 1.5647 - val_accuracy: 0.7548 - val_loss: 1.5639\n",
      "Epoch 16/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7992 - loss: 1.4731 - val_accuracy: 0.7596 - val_loss: 1.5241\n",
      "Epoch 17/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7840 - loss: 1.4444 - val_accuracy: 0.7452 - val_loss: 1.4653\n",
      "Epoch 18/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7790 - loss: 1.4280 - val_accuracy: 0.7740 - val_loss: 1.3989\n",
      "Epoch 19/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7637 - loss: 1.3752 - val_accuracy: 0.7692 - val_loss: 1.3606\n",
      "Epoch 20/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7819 - loss: 1.2933 - val_accuracy: 0.7452 - val_loss: 1.4257\n",
      "Epoch 21/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7602 - loss: 1.3588 - val_accuracy: 0.7404 - val_loss: 1.3090\n",
      "Epoch 22/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7613 - loss: 1.2700 - val_accuracy: 0.7596 - val_loss: 1.2997\n",
      "Epoch 23/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7669 - loss: 1.2161 - val_accuracy: 0.7596 - val_loss: 1.2351\n",
      "Epoch 24/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8055 - loss: 1.1613 - val_accuracy: 0.7740 - val_loss: 1.2729\n",
      "Epoch 25/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7466 - loss: 1.2228 - val_accuracy: 0.7788 - val_loss: 1.2132\n",
      "Epoch 26/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7903 - loss: 1.1335 - val_accuracy: 0.7933 - val_loss: 1.1656\n",
      "Epoch 27/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7846 - loss: 1.0973 - val_accuracy: 0.7452 - val_loss: 1.1584\n",
      "Epoch 28/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7317 - loss: 1.1573 - val_accuracy: 0.7837 - val_loss: 1.1499\n",
      "Epoch 29/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7938 - loss: 1.0488 - val_accuracy: 0.7356 - val_loss: 1.1173\n",
      "Epoch 30/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7721 - loss: 1.0291 - val_accuracy: 0.7885 - val_loss: 1.0693\n",
      "Epoch 31/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7798 - loss: 1.0421 - val_accuracy: 0.7548 - val_loss: 1.0524\n",
      "Epoch 32/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7866 - loss: 0.9865 - val_accuracy: 0.7548 - val_loss: 1.0288\n",
      "Epoch 33/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7856 - loss: 0.9963 - val_accuracy: 0.7933 - val_loss: 1.0088\n",
      "Epoch 34/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7502 - loss: 0.9983 - val_accuracy: 0.7837 - val_loss: 0.9791\n",
      "Epoch 35/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7650 - loss: 0.9742 - val_accuracy: 0.7788 - val_loss: 1.0402\n",
      "Epoch 36/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7828 - loss: 0.9540 - val_accuracy: 0.7548 - val_loss: 0.9915\n",
      "Epoch 37/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7982 - loss: 0.9254 - val_accuracy: 0.7788 - val_loss: 0.9396\n",
      "Epoch 38/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7802 - loss: 0.9171 - val_accuracy: 0.7837 - val_loss: 0.9502\n",
      "Epoch 39/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7924 - loss: 0.9113 - val_accuracy: 0.7740 - val_loss: 0.9485\n",
      "Epoch 40/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8245 - loss: 0.8413 - val_accuracy: 0.8029 - val_loss: 0.9133\n",
      "Epoch 41/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8062 - loss: 0.8663 - val_accuracy: 0.7885 - val_loss: 0.8993\n",
      "Epoch 42/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7477 - loss: 0.9084 - val_accuracy: 0.7596 - val_loss: 0.9195\n",
      "Epoch 43/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8006 - loss: 0.8471 - val_accuracy: 0.8029 - val_loss: 0.8657\n",
      "Epoch 44/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7697 - loss: 0.8799 - val_accuracy: 0.8077 - val_loss: 0.8588\n",
      "Epoch 45/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8365 - loss: 0.7711 - val_accuracy: 0.7933 - val_loss: 0.8737\n",
      "Epoch 46/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7731 - loss: 0.8427 - val_accuracy: 0.7837 - val_loss: 0.8722\n",
      "Epoch 47/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7915 - loss: 0.8246 - val_accuracy: 0.7740 - val_loss: 0.8521\n",
      "Epoch 48/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7991 - loss: 0.7972 - val_accuracy: 0.7596 - val_loss: 0.8548\n",
      "Epoch 49/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8072 - loss: 0.7983 - val_accuracy: 0.7837 - val_loss: 0.8128\n",
      "Epoch 50/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7898 - loss: 0.7972 - val_accuracy: 0.7788 - val_loss: 0.8299\n",
      "Epoch 51/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7692 - loss: 0.8367 - val_accuracy: 0.7885 - val_loss: 0.8147\n",
      "Epoch 52/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8004 - loss: 0.7663 - val_accuracy: 0.8125 - val_loss: 0.8187\n",
      "Epoch 53/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8195 - loss: 0.7303 - val_accuracy: 0.8029 - val_loss: 0.7886\n",
      "Epoch 54/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7621 - loss: 0.7974 - val_accuracy: 0.7356 - val_loss: 0.8845\n",
      "Epoch 55/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7888 - loss: 0.8007 - val_accuracy: 0.7308 - val_loss: 0.9374\n",
      "Epoch 56/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7716 - loss: 0.8289 - val_accuracy: 0.7981 - val_loss: 0.8098\n",
      "Epoch 57/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7923 - loss: 0.7477 - val_accuracy: 0.7837 - val_loss: 0.8148\n",
      "Epoch 58/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7986 - loss: 0.7496 - val_accuracy: 0.7933 - val_loss: 0.7838\n",
      "Epoch 59/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8154 - loss: 0.7429 - val_accuracy: 0.7788 - val_loss: 0.7896\n",
      "Epoch 60/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8030 - loss: 0.7404 - val_accuracy: 0.7885 - val_loss: 0.7684\n",
      "Epoch 61/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7962 - loss: 0.7437 - val_accuracy: 0.7740 - val_loss: 0.7826\n",
      "Epoch 62/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7839 - loss: 0.7436 - val_accuracy: 0.7308 - val_loss: 0.9218\n",
      "Epoch 63/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7517 - loss: 0.8160 - val_accuracy: 0.7788 - val_loss: 0.8099\n",
      "Epoch 64/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7859 - loss: 0.7568 - val_accuracy: 0.7933 - val_loss: 0.7558\n",
      "Epoch 65/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8041 - loss: 0.7020 - val_accuracy: 0.7981 - val_loss: 0.7738\n",
      "Epoch 66/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8201 - loss: 0.7267 - val_accuracy: 0.7548 - val_loss: 0.8068\n",
      "Epoch 67/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7976 - loss: 0.7443 - val_accuracy: 0.7837 - val_loss: 0.7536\n",
      "Epoch 68/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8082 - loss: 0.6962 - val_accuracy: 0.7837 - val_loss: 0.7763\n",
      "Epoch 69/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8173 - loss: 0.6967 - val_accuracy: 0.7788 - val_loss: 0.7483\n",
      "Epoch 70/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7986 - loss: 0.7297 - val_accuracy: 0.7500 - val_loss: 0.8438\n",
      "Epoch 71/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8173 - loss: 0.6967 - val_accuracy: 0.7788 - val_loss: 0.7785\n",
      "Epoch 72/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7943 - loss: 0.7048 - val_accuracy: 0.7692 - val_loss: 0.7736\n",
      "Epoch 73/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7853 - loss: 0.7368 - val_accuracy: 0.7788 - val_loss: 0.7611\n",
      "Epoch 74/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8022 - loss: 0.7069 - val_accuracy: 0.7981 - val_loss: 0.7390\n",
      "Epoch 75/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8139 - loss: 0.6971 - val_accuracy: 0.7740 - val_loss: 0.7449\n",
      "Epoch 76/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8111 - loss: 0.6884 - val_accuracy: 0.7404 - val_loss: 0.7973\n",
      "Epoch 77/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8172 - loss: 0.6726 - val_accuracy: 0.7981 - val_loss: 0.7316\n",
      "Epoch 78/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8199 - loss: 0.6724 - val_accuracy: 0.7740 - val_loss: 0.7649\n",
      "Epoch 79/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8032 - loss: 0.7293 - val_accuracy: 0.7885 - val_loss: 0.7672\n",
      "Epoch 80/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8206 - loss: 0.6850 - val_accuracy: 0.7837 - val_loss: 0.7423\n",
      "Epoch 81/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8041 - loss: 0.7084 - val_accuracy: 0.8029 - val_loss: 0.7153\n",
      "Epoch 82/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8259 - loss: 0.6822 - val_accuracy: 0.7837 - val_loss: 0.7177\n",
      "Epoch 83/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8148 - loss: 0.6827 - val_accuracy: 0.7981 - val_loss: 0.7084\n",
      "Epoch 84/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8231 - loss: 0.6427 - val_accuracy: 0.7885 - val_loss: 0.6995\n",
      "Epoch 85/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8384 - loss: 0.6594 - val_accuracy: 0.7740 - val_loss: 0.7974\n",
      "Epoch 86/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8117 - loss: 0.6882 - val_accuracy: 0.8221 - val_loss: 0.7139\n",
      "Epoch 87/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8080 - loss: 0.6887 - val_accuracy: 0.8077 - val_loss: 0.7071\n",
      "Epoch 88/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8205 - loss: 0.6741 - val_accuracy: 0.8029 - val_loss: 0.7411\n",
      "Epoch 89/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8249 - loss: 0.6872 - val_accuracy: 0.8125 - val_loss: 0.7171\n",
      "Epoch 90/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8317 - loss: 0.6606 - val_accuracy: 0.8173 - val_loss: 0.7291\n",
      "Epoch 91/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8308 - loss: 0.6513 - val_accuracy: 0.8317 - val_loss: 0.7000\n",
      "Epoch 92/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8206 - loss: 0.6636 - val_accuracy: 0.7740 - val_loss: 0.7622\n",
      "Epoch 93/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7949 - loss: 0.7131 - val_accuracy: 0.7837 - val_loss: 0.7483\n",
      "Epoch 94/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7936 - loss: 0.7039 - val_accuracy: 0.8317 - val_loss: 0.6986\n",
      "Epoch 95/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8183 - loss: 0.6501 - val_accuracy: 0.8029 - val_loss: 0.6989\n",
      "Epoch 96/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8282 - loss: 0.6401 - val_accuracy: 0.7308 - val_loss: 0.8445\n",
      "Epoch 97/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7987 - loss: 0.7263 - val_accuracy: 0.8269 - val_loss: 0.6999\n",
      "Epoch 98/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8189 - loss: 0.6659 - val_accuracy: 0.7981 - val_loss: 0.7086\n",
      "Epoch 99/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8065 - loss: 0.6759 - val_accuracy: 0.7933 - val_loss: 0.7277\n",
      "Epoch 100/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8150 - loss: 0.6700 - val_accuracy: 0.7163 - val_loss: 0.8553\n",
      "Epoch 101/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8027 - loss: 0.6891 - val_accuracy: 0.8173 - val_loss: 0.7069\n",
      "Epoch 102/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7949 - loss: 0.6794 - val_accuracy: 0.8029 - val_loss: 0.7061\n",
      "Epoch 103/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8455 - loss: 0.6239 - val_accuracy: 0.7933 - val_loss: 0.7574\n",
      "Epoch 104/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8035 - loss: 0.7141 - val_accuracy: 0.7885 - val_loss: 0.6911\n",
      "Epoch 105/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8082 - loss: 0.6771 - val_accuracy: 0.8125 - val_loss: 0.6932\n",
      "Epoch 106/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7964 - loss: 0.6726 - val_accuracy: 0.7596 - val_loss: 0.7551\n",
      "Epoch 107/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8159 - loss: 0.6491 - val_accuracy: 0.7981 - val_loss: 0.7255\n",
      "Epoch 108/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7972 - loss: 0.6819 - val_accuracy: 0.7692 - val_loss: 0.7243\n",
      "Epoch 109/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8163 - loss: 0.6491 - val_accuracy: 0.8077 - val_loss: 0.6959\n",
      "Epoch 110/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8506 - loss: 0.6004 - val_accuracy: 0.8125 - val_loss: 0.6907\n",
      "Epoch 111/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8433 - loss: 0.6296 - val_accuracy: 0.8077 - val_loss: 0.6977\n",
      "Epoch 112/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8490 - loss: 0.6183 - val_accuracy: 0.8077 - val_loss: 0.7003\n",
      "Epoch 113/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8147 - loss: 0.6520 - val_accuracy: 0.8077 - val_loss: 0.7083\n",
      "Epoch 114/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8239 - loss: 0.6821 - val_accuracy: 0.8462 - val_loss: 0.6709\n",
      "Epoch 115/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8222 - loss: 0.6458 - val_accuracy: 0.7981 - val_loss: 0.7136\n",
      "Epoch 116/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8193 - loss: 0.6570 - val_accuracy: 0.7933 - val_loss: 0.7252\n",
      "Epoch 117/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8151 - loss: 0.6576 - val_accuracy: 0.7981 - val_loss: 0.6715\n",
      "Epoch 118/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8626 - loss: 0.5926 - val_accuracy: 0.8221 - val_loss: 0.6927\n",
      "Epoch 119/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8526 - loss: 0.5914 - val_accuracy: 0.7500 - val_loss: 0.7930\n",
      "Epoch 120/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8266 - loss: 0.6458 - val_accuracy: 0.8221 - val_loss: 0.6765\n",
      "Epoch 121/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8335 - loss: 0.6447 - val_accuracy: 0.8077 - val_loss: 0.6768\n",
      "Epoch 122/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8322 - loss: 0.6283 - val_accuracy: 0.8221 - val_loss: 0.6907\n",
      "Epoch 123/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8113 - loss: 0.6461 - val_accuracy: 0.8173 - val_loss: 0.6987\n",
      "Epoch 124/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8704 - loss: 0.5538 - val_accuracy: 0.8221 - val_loss: 0.6928\n",
      "Epoch 125/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8133 - loss: 0.6596 - val_accuracy: 0.8125 - val_loss: 0.6920\n",
      "Epoch 126/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8228 - loss: 0.6588 - val_accuracy: 0.7933 - val_loss: 0.6906\n",
      "Epoch 127/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8183 - loss: 0.6103 - val_accuracy: 0.8125 - val_loss: 0.6686\n",
      "Epoch 128/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8232 - loss: 0.6307 - val_accuracy: 0.7933 - val_loss: 0.7548\n",
      "Epoch 129/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7927 - loss: 0.6803 - val_accuracy: 0.8221 - val_loss: 0.6874\n",
      "Epoch 130/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8238 - loss: 0.6358 - val_accuracy: 0.8077 - val_loss: 0.6666\n",
      "Epoch 131/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8726 - loss: 0.5765 - val_accuracy: 0.8317 - val_loss: 0.6593\n",
      "Epoch 132/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8346 - loss: 0.6441 - val_accuracy: 0.8173 - val_loss: 0.6686\n",
      "Epoch 133/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8058 - loss: 0.6788 - val_accuracy: 0.8125 - val_loss: 0.6897\n",
      "Epoch 134/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8340 - loss: 0.6257 - val_accuracy: 0.7740 - val_loss: 0.6936\n",
      "Epoch 135/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8130 - loss: 0.6195 - val_accuracy: 0.7837 - val_loss: 0.7180\n",
      "Epoch 136/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8366 - loss: 0.6146 - val_accuracy: 0.8029 - val_loss: 0.6936\n",
      "Epoch 137/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7983 - loss: 0.6600 - val_accuracy: 0.8173 - val_loss: 0.6508\n",
      "Epoch 138/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8654 - loss: 0.5651 - val_accuracy: 0.7885 - val_loss: 0.7096\n",
      "Epoch 139/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8375 - loss: 0.6169 - val_accuracy: 0.8221 - val_loss: 0.6790\n",
      "Epoch 140/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8460 - loss: 0.5877 - val_accuracy: 0.8510 - val_loss: 0.6433\n",
      "Epoch 141/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8625 - loss: 0.5769 - val_accuracy: 0.7500 - val_loss: 0.7853\n",
      "Epoch 142/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8347 - loss: 0.6450 - val_accuracy: 0.7981 - val_loss: 0.7220\n",
      "Epoch 143/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8590 - loss: 0.5882 - val_accuracy: 0.8365 - val_loss: 0.6691\n",
      "Epoch 144/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8586 - loss: 0.5636 - val_accuracy: 0.8077 - val_loss: 0.6885\n",
      "Epoch 145/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8490 - loss: 0.6064 - val_accuracy: 0.8365 - val_loss: 0.6731\n",
      "Epoch 146/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8492 - loss: 0.5771 - val_accuracy: 0.7981 - val_loss: 0.7095\n",
      "Epoch 147/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8276 - loss: 0.6471 - val_accuracy: 0.7644 - val_loss: 0.7725\n",
      "Epoch 148/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8633 - loss: 0.6012 - val_accuracy: 0.8125 - val_loss: 0.6832\n",
      "Epoch 149/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8689 - loss: 0.5514 - val_accuracy: 0.7933 - val_loss: 0.6985\n",
      "Epoch 150/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8475 - loss: 0.6210 - val_accuracy: 0.8317 - val_loss: 0.6629\n",
      "Epoch 151/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8571 - loss: 0.5852 - val_accuracy: 0.8125 - val_loss: 0.7302\n",
      "Epoch 152/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8077 - loss: 0.6650 - val_accuracy: 0.7981 - val_loss: 0.7032\n",
      "Epoch 153/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8488 - loss: 0.5957 - val_accuracy: 0.8029 - val_loss: 0.6879\n",
      "Epoch 154/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8404 - loss: 0.5888 - val_accuracy: 0.7981 - val_loss: 0.6763\n",
      "Epoch 155/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8415 - loss: 0.5917 - val_accuracy: 0.8221 - val_loss: 0.6827\n",
      "Epoch 156/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8178 - loss: 0.6373 - val_accuracy: 0.8077 - val_loss: 0.6872\n",
      "Epoch 157/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8466 - loss: 0.5932 - val_accuracy: 0.8029 - val_loss: 0.7302\n",
      "Epoch 158/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8351 - loss: 0.5876 - val_accuracy: 0.7885 - val_loss: 0.7615\n",
      "Epoch 159/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8108 - loss: 0.6783 - val_accuracy: 0.8269 - val_loss: 0.6505\n",
      "Epoch 160/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8773 - loss: 0.5554 - val_accuracy: 0.8269 - val_loss: 0.6712\n",
      "Epoch 161/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8410 - loss: 0.5941 - val_accuracy: 0.7788 - val_loss: 0.7562\n",
      "Epoch 162/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8483 - loss: 0.5898 - val_accuracy: 0.8029 - val_loss: 0.7140\n",
      "Epoch 163/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8342 - loss: 0.6143 - val_accuracy: 0.8125 - val_loss: 0.6459\n",
      "Epoch 164/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8361 - loss: 0.5973 - val_accuracy: 0.8125 - val_loss: 0.6770\n",
      "Epoch 165/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8757 - loss: 0.5643 - val_accuracy: 0.8317 - val_loss: 0.6590\n",
      "Epoch 166/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8416 - loss: 0.5851 - val_accuracy: 0.8221 - val_loss: 0.6786\n",
      "Epoch 167/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8457 - loss: 0.6343 - val_accuracy: 0.8029 - val_loss: 0.6913\n",
      "Epoch 168/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8616 - loss: 0.5827 - val_accuracy: 0.7885 - val_loss: 0.7046\n",
      "Epoch 169/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8809 - loss: 0.5309 - val_accuracy: 0.8125 - val_loss: 0.6597\n",
      "Epoch 170/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8740 - loss: 0.5475 - val_accuracy: 0.8221 - val_loss: 0.6693\n",
      "Epoch 171/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8614 - loss: 0.5320 - val_accuracy: 0.7837 - val_loss: 0.7260\n",
      "Epoch 172/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8535 - loss: 0.5926 - val_accuracy: 0.7788 - val_loss: 0.7994\n",
      "Epoch 173/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8385 - loss: 0.6182 - val_accuracy: 0.8221 - val_loss: 0.6590\n",
      "Epoch 174/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8684 - loss: 0.5786 - val_accuracy: 0.7596 - val_loss: 0.7501\n",
      "Epoch 175/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8139 - loss: 0.6515 - val_accuracy: 0.8077 - val_loss: 0.7044\n",
      "Epoch 176/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8471 - loss: 0.6175 - val_accuracy: 0.8221 - val_loss: 0.6678\n",
      "Epoch 177/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8599 - loss: 0.5685 - val_accuracy: 0.8125 - val_loss: 0.6764\n",
      "Epoch 178/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8303 - loss: 0.5968 - val_accuracy: 0.8029 - val_loss: 0.6781\n",
      "Epoch 179/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8437 - loss: 0.5816 - val_accuracy: 0.7788 - val_loss: 0.7332\n",
      "Epoch 180/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8410 - loss: 0.5728 - val_accuracy: 0.8077 - val_loss: 0.7430\n",
      "Epoch 181/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8611 - loss: 0.5600 - val_accuracy: 0.8317 - val_loss: 0.6496\n",
      "Epoch 182/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8508 - loss: 0.5860 - val_accuracy: 0.7596 - val_loss: 0.7890\n",
      "Epoch 183/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8429 - loss: 0.6170 - val_accuracy: 0.8077 - val_loss: 0.6744\n",
      "Epoch 184/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8645 - loss: 0.5668 - val_accuracy: 0.7933 - val_loss: 0.7449\n",
      "Epoch 185/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8228 - loss: 0.5912 - val_accuracy: 0.8173 - val_loss: 0.6698\n",
      "Epoch 186/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8617 - loss: 0.5515 - val_accuracy: 0.7067 - val_loss: 0.8276\n",
      "Epoch 187/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8129 - loss: 0.6300 - val_accuracy: 0.7981 - val_loss: 0.7758\n",
      "Epoch 188/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8495 - loss: 0.5606 - val_accuracy: 0.7885 - val_loss: 0.7614\n",
      "Epoch 189/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8544 - loss: 0.5936 - val_accuracy: 0.8125 - val_loss: 0.6866\n",
      "Epoch 190/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8433 - loss: 0.5587 - val_accuracy: 0.8077 - val_loss: 0.6920\n",
      "Epoch 191/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8672 - loss: 0.5339 - val_accuracy: 0.7356 - val_loss: 0.8208\n",
      "Epoch 192/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8297 - loss: 0.6136 - val_accuracy: 0.8029 - val_loss: 0.7017\n",
      "Epoch 193/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8582 - loss: 0.5657 - val_accuracy: 0.8125 - val_loss: 0.6611\n",
      "Epoch 194/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8461 - loss: 0.5644 - val_accuracy: 0.8077 - val_loss: 0.6627\n",
      "Epoch 195/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8703 - loss: 0.5304 - val_accuracy: 0.8077 - val_loss: 0.7000\n",
      "Epoch 196/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8557 - loss: 0.5846 - val_accuracy: 0.8125 - val_loss: 0.6589\n",
      "Epoch 197/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8669 - loss: 0.5477 - val_accuracy: 0.8029 - val_loss: 0.6448\n",
      "Epoch 198/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8817 - loss: 0.5369 - val_accuracy: 0.7740 - val_loss: 0.7105\n",
      "Epoch 199/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8700 - loss: 0.5553 - val_accuracy: 0.8269 - val_loss: 0.6684\n",
      "Epoch 200/200\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8728 - loss: 0.5375 - val_accuracy: 0.8125 - val_loss: 0.6502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/07/23 10:51:46 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train,y_train,epochs = 200, validation_data=(X_test,y_test))\n",
    "\n",
    "for epoch in range(len(history.history['loss'])):\n",
    "    training_loss = history.history['loss'][epoch]\n",
    "    validation_loss = history.history['val_loss'][epoch]\n",
    "    validation_accuracy = history.history['val_accuracy'][epoch]\n",
    "    training_accuracy = history.history['accuracy'][epoch]\n",
    "\n",
    "    mlflow.log_metric('training__loss', training_loss, step = epoch)\n",
    "    mlflow.log_metric('training_accuracy', training_accuracy, step = epoch)\n",
    "    mlflow.log_metric('validation_accuracy',validation_accuracy,step=epoch)\n",
    "    mlflow.log_metric('validation_loss',validation_loss, step=epoch)\n",
    "\n",
    "mlflow.tensorflow.log_model(model,'model')\n",
    "mlflow.log_artifact('/home/dylan/HealthDetectionThroughVoice/HealthDetection.ipynb','source_code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(model,open('nn_model.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
